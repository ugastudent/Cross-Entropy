{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, cost is  1.34345264825\n",
      "Training epoch 10, cost is  1.07437523984\n",
      "Training epoch 20, cost is  0.952266180083\n",
      "Training epoch 30, cost is  0.889850480857\n",
      "Training epoch 40, cost is  0.855747695272\n",
      "Training epoch 50, cost is  0.83639485871\n",
      "Training epoch 60, cost is  0.825168827213\n",
      "Training epoch 70, cost is  0.81857263403\n",
      "Training epoch 80, cost is  0.814667264454\n",
      "Training epoch 90, cost is  0.81234456785\n",
      "Training epoch 100, cost is  0.810959433707\n",
      "Training epoch 110, cost is  0.810132083934\n",
      "Training epoch 120, cost is  0.809637427493\n",
      "Training epoch 130, cost is  0.809341511987\n",
      "Training epoch 140, cost is  0.809164427263\n",
      "Training epoch 150, cost is  0.809058432637\n",
      "Training epoch 160, cost is  0.808994981401\n",
      "Training epoch 170, cost is  0.808956994982\n",
      "Training epoch 180, cost is  0.808934252608\n",
      "Training epoch 190, cost is  0.808920636445\n",
      "Training epoch 200, cost is  0.808912484138\n",
      "Training epoch 210, cost is  0.80890760312\n",
      "Training epoch 220, cost is  0.808904680699\n",
      "Training epoch 230, cost is  0.808902930946\n",
      "Training epoch 240, cost is  0.808901883308\n",
      "Training epoch 250, cost is  0.808901256049\n",
      "Training epoch 260, cost is  0.808900880486\n",
      "Training epoch 270, cost is  0.808900655623\n",
      "Training epoch 280, cost is  0.80890052099\n",
      "Training epoch 290, cost is  0.808900440379\n",
      "Training epoch 300, cost is  0.808900392115\n",
      "Training epoch 310, cost is  0.808900363218\n",
      "Training epoch 320, cost is  0.808900345915\n",
      "Training epoch 330, cost is  0.808900335556\n",
      "Training epoch 340, cost is  0.808900329354\n",
      "Training epoch 350, cost is  0.80890032564\n",
      "Training epoch 360, cost is  0.808900323416\n",
      "Training epoch 370, cost is  0.808900322085\n",
      "Training epoch 380, cost is  0.808900321288\n",
      "Training epoch 390, cost is  0.808900320811\n",
      "Training epoch 400, cost is  0.808900320525\n",
      "Training epoch 410, cost is  0.808900320354\n",
      "Training epoch 420, cost is  0.808900320251\n",
      "Training epoch 430, cost is  0.80890032019\n",
      "Training epoch 440, cost is  0.808900320153\n",
      "Training epoch 450, cost is  0.808900320131\n",
      "Training epoch 460, cost is  0.808900320118\n",
      "Training epoch 470, cost is  0.80890032011\n",
      "Training epoch 480, cost is  0.808900320106\n",
      "Training epoch 490, cost is  0.808900320103\n",
      "Training epoch 500, cost is  0.808900320101\n",
      "Training epoch 510, cost is  0.8089003201\n",
      "Training epoch 520, cost is  0.808900320099\n",
      "Training epoch 530, cost is  0.808900320099\n",
      "Training epoch 540, cost is  0.808900320099\n",
      "Training epoch 550, cost is  0.808900320099\n",
      "Training epoch 560, cost is  0.808900320099\n",
      "Training epoch 570, cost is  0.808900320099\n",
      "Training epoch 580, cost is  0.808900320099\n",
      "Training epoch 590, cost is  0.808900320099\n",
      "<ipykernel.iostream.OutStream object at 0x00000212F6688470> [ 0.69202073  0.30797927]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    " Logistic Regression\n",
    " \n",
    " References :\n",
    "    - Jason Rennie: Logistic Regression,\n",
    "   http://qwone.com/~jason/writing/lr.pdf\n",
    " \n",
    "   - DeepLearningTutorials\n",
    "   https://github.com/lisa-lab/DeepLearningTutorials\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "\n",
    "numpy.seterr(all='ignore')\n",
    " \n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + numpy.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e = numpy.exp(x - numpy.max(x))  # prevent overflow\n",
    "    if e.ndim == 1:\n",
    "        return e / numpy.sum(e, axis=0)\n",
    "    else:  \n",
    "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, label, n_in, n_out):\n",
    "        self.x = input\n",
    "        self.y = label\n",
    "        self.W = numpy.zeros((n_in, n_out))  # initialize W 0\n",
    "        self.b = numpy.zeros(n_out)          # initialize bias 0\n",
    "\n",
    "        # self.params = [self.W, self.b]\n",
    "\n",
    "    def train(self, lr=0.1, input=None, L2_reg=0.00):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        # p_y_given_x = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
    "        p_y_given_x = softmax(numpy.dot(self.x, self.W) + self.b)\n",
    "        d_y = self.y - p_y_given_x\n",
    "        \n",
    "        self.W += lr * numpy.dot(self.x.T, d_y) - lr * L2_reg * self.W\n",
    "        self.b += lr * numpy.mean(d_y, axis=0)\n",
    "        \n",
    "        # cost = self.negative_log_likelihood()\n",
    "        # return cost\n",
    "\n",
    "    def negative_log_likelihood(self):\n",
    "        # sigmoid_activation = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
    "        sigmoid_activation = softmax(numpy.dot(self.x, self.W) + self.b)\n",
    "\n",
    "        cross_entropy = - numpy.mean(\n",
    "            numpy.sum(self.y * numpy.log(sigmoid_activation) +\n",
    "            (1 - self.y) * numpy.log(1 - sigmoid_activation),\n",
    "                      axis=1))\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # return sigmoid(numpy.dot(x, self.W) + self.b)\n",
    "        return softmax(numpy.dot(x, self.W) + self.b)\n",
    "\n",
    "\n",
    "def test_lr(learning_rate=0.01, n_epochs=200):\n",
    "    # training data\n",
    "    x = numpy.array([[1,1,1,0,0,0],\n",
    "                     [1,0,1,0,0,0],\n",
    "                     [1,1,1,0,0,0],\n",
    "                     [0,0,1,1,1,0],\n",
    "                     [0,0,1,1,0,0],\n",
    "                     [0,0,1,1,1,0]])\n",
    "    y = numpy.array([[1, 0],\n",
    "                     [1, 0],\n",
    "                     [1, 0],\n",
    "                     [0, 1],\n",
    "                     [0, 1],\n",
    "                     [0, 1]])\n",
    "\n",
    "\n",
    "    # construct LogisticRegression\n",
    "    classifier = LogisticRegression(input=x, label=y, n_in=6, n_out=2)\n",
    "    cost=1\n",
    "    # train\n",
    "    for epoch in range(600):\n",
    "        classifier.train(lr=learning_rate)\n",
    "        cost = classifier.negative_log_likelihood()\n",
    "        if epoch%10==0:\n",
    "            print (  'Training epoch %d, cost is ' % epoch, cost)\n",
    "        learning_rate *= 0.95\n",
    "\n",
    "\n",
    "    # test\n",
    "    x = numpy.array([1, 1, 0, 0, 0, 0])\n",
    "    print ( sys.stderr, classifier.predict(x))\n",
    "    cost = classifier.negative_log_likelihood()\n",
    "if __name__ == \"__main__\":\n",
    "    test_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[title]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
